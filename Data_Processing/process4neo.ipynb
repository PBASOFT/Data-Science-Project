{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipynb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "relevant-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from NER import get_entities\n",
    "from sentiment_analysis import predictSentiment\n",
    "import pandas as pd \n",
    "import datetime\n",
    "import requests\n",
    "from collections import Counter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedicated-export",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictSentiment(\"i love this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "motivated-pepper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GME']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_entities('this GME is making me rich')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-republican",
   "metadata": {},
   "source": [
    "### Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "streaming-melbourne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_clean = pd.read_csv('data_files/wallstreetbets_21.csv', lineterminator='\\n')\n",
    "data = pd.read_csv('../Data_Collecting/data_files/Stocks_34.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_filenames = []\n",
    "#old_filenames.append('..Data_Collecting/data_files/wallstreetbets_50.csv')\n",
    "#old_filenames.append('..Data_Collecting/data_files/wallstreetbets_44.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments = []\n",
    "#submissions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter = 50\n",
    "#for c in comments:\n",
    "#    c.to_csv('./data_files/comment_sample'+str(counter)+'.csv')\n",
    "#    counter +=1\n",
    "#for s in submissions:\n",
    "#    s.to_csv('./data_files/submission_sample'+str(counter)+'.csv')\n",
    "#    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-offense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "democratic-smart",
   "metadata": {},
   "source": [
    "# Sample for neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-alexandria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_files/submission_sample322.csv\n",
      "Submissions are on their way to Neo4J\n",
      "./data_files/submission_sample256.csv\n",
      "Submissions are on their way to Neo4J\n",
      "./data_files/comment_sample92.csv\n",
      "Comments are on their way to Neo4J\n",
      "./data_files/comment_sample86.csv\n"
     ]
    }
   ],
   "source": [
    "for file_name in glob.glob('./data_files/'+'*.csv'):\n",
    "    print(file_name)\n",
    "    data = pd.read_csv(file_name,lineterminator='\\n',converters={'Organizations': eval})\n",
    "    if file_name[13:].startswith('s'):\n",
    "        print('Submissions are on their way to Neo4J')\n",
    "        insert_submissions(data)\n",
    "    else:\n",
    "        print('Comments are on their way to Neo4J')\n",
    "        insert_comments(data) # 16.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments[5:55]\n",
    "df_comments.to_csv('comments_sample_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-optimum",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-category",
   "metadata": {},
   "source": [
    "## Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "military-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list with submissions dataframe and comments dataframe\n",
    "def transform_reddit_data(data: pd.DataFrame):\n",
    "        dfs = []\n",
    "        # submissions\n",
    "        #candidates =['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title']\n",
    "        #df_submissions = data.drop([x for x in candidates if x in data.columns], axis=1)\n",
    "        df_submissions = data.drop(['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title'],axis=1)\n",
    "        df_submissions['created'] = df_submissions['created'].apply(datetime_to_date)\n",
    "        df_submissions = df_submissions.dropna()\n",
    "        df_submissions.rename(columns = {'post_id':'id'}, inplace = True)\n",
    "        df_submissions['Organizations'] = df_submissions['text'].apply(get_entities).apply(clean_orgs)\n",
    "        df_submissions['Sentiment'] = df_submissions['text'].apply(predictSentiment)\n",
    "        dfs.append(df_submissions)\n",
    "        # comments\n",
    "        #candidates =['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title']\n",
    "        #df_comments = data.drop([x for x in candidates if x in data.columns], axis=1)\n",
    "        df_comments = data.drop(['Unnamed: 0','post_id', 'fullname', 'title'],axis=1)\n",
    "        df_comments['created'] = df_comments['created'].apply(datetime_to_date)\n",
    "        df_comments = df_comments.dropna()\n",
    "        df_comments['Organizations'] = df_comments['text'].apply(get_entities).apply(clean_orgs)\n",
    "        df_comments['Sentiment'] = df_comments['text'].apply(predictSentiment)\n",
    "        dfs.append(df_comments)\n",
    "        \n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "shared-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes utc timestamp to datetime.date\n",
    "def datetime_to_date(timestamp):\n",
    "    return pd.to_datetime(timestamp).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "killing-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mentions that we are interested in\n",
    "selected_orgs =  ['HKG', 'Alibaba','AMC', 'Palantir Technologies', 'PLTR', 'FORD', 'Lordstown Motors', 'RIDE', 'Virgin Galactic', 'SPCE', 'AI', 'C3.AI', 'TSLA', 'GE', 'GME', 'AAPL', 'Tesla', 'Apple', 'General Electric', 'GE', 'NOK', 'Nokia']\n",
    "orgs_dict = {\"Alibaba\":\"HKG\",\"AMC\":\"AMC\",\"Palantir Technologies\":\"PLTR\",\"FORD\":\"FORD\", \"Lordstown Motors\":\"RIDE\",\"Virgin Galactic\":\"SPCE\",\"c3.AI\":\"AI\",\"Tesla\":\"TSLA\", \"General Electric\":\"GE\",\"Apple\":\"AAPL\",\"GameStop\":\"GME\",\"Gamestop\":\"GME\", \"Nokia\":\"NOK\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-median",
   "metadata": {},
   "source": [
    "### Clean orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "convinced-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning a list of mentioned tickers\n",
    "def clean_orgs(organizations):\n",
    "    orgs = []\n",
    "    for org in organizations:\n",
    "        if org in selected_orgs:\n",
    "            if org in orgs_dict:\n",
    "                org = orgs_dict[org]\n",
    "                orgs.append(org)\n",
    "            else: \n",
    "                orgs.append(org)\n",
    "    for org in orgs:\n",
    "        o = set(orgs)\n",
    "        orgs = list(o)\n",
    "    return orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "homeless-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPCE']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_orgs(['Virgin Galactic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-right",
   "metadata": {},
   "source": [
    "## A Look at the mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-planner",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creates a list of mentionend entities\n",
    "\n",
    "#data['Organizations'] = data['text'].apply(get_entities)\n",
    "#orgs = data['Organizations'].to_list()\n",
    "#orgs_flat = [org for sublist in orgs for org in sublist] # Pulls out entities from the nested lists in orgs => new flat list\n",
    "# Print 20 most mentions ORGs\n",
    "#from collections import Counter\n",
    "#org_freq = Counter(orgs_flat)\n",
    "#org_freq.most_common(20)                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-graduate",
   "metadata": {},
   "source": [
    "## Transform and save the data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stainless-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submissionsransforming into lists of submissions and comments\n",
    "#submissions = pd.read_csv('submission_sample0.csv', converters={'Organizations': eval})\n",
    "#dfs = transform_reddit_data(data)\n",
    "#submissions = dfs[0]\n",
    "#comments = dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "czech-rally",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "********\n",
      "AAPL\n",
      "********\n",
      "********\n",
      "AAPL\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "## SUBMISSIONS subset\n",
    "#print(len(submissions))\n",
    "#df = submissions[1:500]\n",
    "#insert_submissions(df)\n",
    "#COMMENTS subset\n",
    "#print(len(comments))\n",
    "#df_c = comments[500:5500]\n",
    "#\n",
    "#insert_comments(df_c)\n",
    "#insert_c(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "anticipated-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://localhost:5050/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "declared-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_submissions(dataframe: pd.DataFrame):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        #create submission nodes\n",
    "        url2 = 'submission/'+row['subreddit']+'/'+ str(row['created'])+'/'+row['id']\n",
    "        response = requests.post(url+url2)\n",
    "        if not response.status_code == 200:\n",
    "            print ('submission id '+ row['id'] + ' failed insertion')\n",
    "        # create mentions relationships\n",
    "        for org in row['Organizations']:\n",
    "            url3 = 'connection/mentions/submission/'+row['id']+'/'+ org + '/' + str(row['created'])\n",
    "            response = requests.post(url+url3)\n",
    "            if not response.status_code == 200:\n",
    "                print ('mentions between '+ row['id'] + ' and ' + org + ' failed creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sustained-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_comments(dataframe: pd.DataFrame):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        #create comment nodes\n",
    "        url2 = 'comment/'+row['subreddit']+'/'+row['created']+'/'+row['comment_id']+'/'+row['parent_id']\n",
    "        response = requests.post(url+url2)\n",
    "        if not response.status_code == 200:\n",
    "            print ('comment id '+ row['comment_id'] + ' failed insertion')\n",
    "            \n",
    "        # create mentions relationships\n",
    "        for org in row['Organizations']:\n",
    "            url3 = 'connection/mentions/comment/'+row['comment_id']+'/'+ org +'/'+ row['created']#.strftime('%m-%d-%Y')\n",
    "            response = requests.post(url+url3)\n",
    "            if not response.status_code == 200:\n",
    "                print ('mention between '+ row['comment_id'] + ' and ' + org + ' failed creation')\n",
    "    \n",
    "           \n",
    "        # create replies relationships\n",
    "        row['parent_id'] = pd.Series(row['parent_id'], dtype=\"string\") #turning parent_id from type Series to String\n",
    "        for parent_id in row['parent_id']:\n",
    "            p_id = parent_id[3:]\n",
    "            if parent_id[:2] == 't1':\n",
    "                url_reply = 'connection/replies/comment/' + row['comment_id'] + '/' + p_id\n",
    "            elif parent_id[:2] == 't3':\n",
    "                url_reply = 'connection/replies/submission/' + row['comment_id'] + '/' + p_id\n",
    "            response = requests.post(url+url_reply)\n",
    "            if not response.status_code == 200:\n",
    "                print ('replies relationship between '+ row['comment_id'] + ' and ' + p_id + ' failed creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-hobby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-blood",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-chase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-basin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-celebrity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all csv files\n",
    "for file_name in glob.glob('../Data_Collecting/data_files/'+'*.csv'):\n",
    "    print('* '+file_name)\n",
    "    if file_name not in old_filenames:\n",
    "        data = pd.read_csv(file_name,lineterminator='\\n')\n",
    "        # Transform data\n",
    "        print('   * Transforming data')\n",
    "        dfs = transform_reddit_data(data)\n",
    "        df_submissions = dfs[0]\n",
    "        df_comments = dfs[1]\n",
    "        # Persist submissions and comments \n",
    "        print('   * Persisting submissions')\n",
    "        insert_submissions(df_submissions)\n",
    "        submissions.append(df_submissions)\n",
    "        print('   * Persisting comments')\n",
    "        insert_comments(df_comments)\n",
    "        comments.append(df_comments)\n",
    "        old_filenames.append(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
