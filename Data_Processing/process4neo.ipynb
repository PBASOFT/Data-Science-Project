{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipynb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "arctic-sport",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from NER.ipynb\n",
      "importing Jupyter notebook from sentiment_analysis.ipynb\n",
      "this\n",
      "is\n",
      "the\n",
      "first\n",
      ".\n",
      "this\n",
      "is\n",
      "the\n",
      "second\n",
      ".\n",
      "this\n",
      "is\n",
      "the\n",
      "third\n",
      "this is the first.\n",
      "this is the second.\n",
      "this is the third\n",
      ".\n",
      "second\n",
      ".\n",
      "run run\n",
      "running run\n",
      "runs run\n",
      "runner runner\n",
      "(1000, 2)\n",
      "(1000, 2)\n",
      "(748, 2)\n",
      "(10, 2)\n",
      "<bound method NDFrame.head of                                                  Review  Sentiment\n",
      "853   The price is reasonable and the service is great.          1\n",
      "1734  We have tried 2 units and they both failed wit...          0\n",
      "1739                    I great reception all the time.          1\n",
      "2105  This is a bad film, with bad writing, and good...          0\n",
      "1294  The voice recognition thru the handset is exce...          1\n",
      "...                                                 ...        ...\n",
      "835   I paid the bill but did not tip because I felt...          0\n",
      "1474                          The delivery was on time.          1\n",
      "684                                    Damn good steak.          1\n",
      "2053             The directing seems too pretentious.            0\n",
      "1142                    I was not happy with this item.          0\n",
      "\n",
      "[2748 rows x 2 columns]>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78       270\n",
      "           1       0.79      0.76      0.78       280\n",
      "\n",
      "    accuracy                           0.78       550\n",
      "   macro avg       0.78      0.78      0.78       550\n",
      "weighted avg       0.78      0.78      0.78       550\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from NER import get_entities\n",
    "from sentiment_analysis import predictSentiment\n",
    "import pandas as pd \n",
    "import datetime\n",
    "import requests\n",
    "from collections import Counter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "floral-wilson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictSentiment(\"i love this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recovered-injection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GME']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities('this GME is making me rich')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-attack",
   "metadata": {},
   "source": [
    "### Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entitled-gamma",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>created</th>\n",
       "      <th>fullname</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gzsuulr</td>\n",
       "      <td>2021-05-28 20:39:11</td>\n",
       "      <td>t1_gzsuulr</td>\n",
       "      <td>t1_gzpc2r8</td>\n",
       "      <td>stocks</td>\n",
       "      <td>They denied me when I asked for a BRK discount :(</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-05-28 20:39:14</td>\n",
       "      <td>t3_nn724o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stocks</td>\n",
       "      <td>I'm sure you know those two has stock splits. ...</td>\n",
       "      <td>nn724o</td>\n",
       "      <td>TTD or NVDA which one is better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>gzsuwbn</td>\n",
       "      <td>2021-05-28 20:39:34</td>\n",
       "      <td>t1_gzsuwbn</td>\n",
       "      <td>t3_nn5tcm</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Mom?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>gzsuyhy</td>\n",
       "      <td>2021-05-28 20:40:03</td>\n",
       "      <td>t1_gzsuyhy</td>\n",
       "      <td>t1_gzrz5t7</td>\n",
       "      <td>stocks</td>\n",
       "      <td>I have medical conditions that they are workin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>gzsuz0n</td>\n",
       "      <td>2021-05-28 20:40:10</td>\n",
       "      <td>t1_gzsuz0n</td>\n",
       "      <td>t1_gzssh7x</td>\n",
       "      <td>stocks</td>\n",
       "      <td>It's daunting at first. A quick list of topics...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>gzv1em4</td>\n",
       "      <td>2021-05-29 11:30:53</td>\n",
       "      <td>t1_gzv1em4</td>\n",
       "      <td>t3_nn724o</td>\n",
       "      <td>stocks</td>\n",
       "      <td>You missed the big run up on TTD after its ear...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>gzv1epl</td>\n",
       "      <td>2021-05-29 11:30:55</td>\n",
       "      <td>t1_gzv1epl</td>\n",
       "      <td>t1_gzu5fmn</td>\n",
       "      <td>stocks</td>\n",
       "      <td>I’ve been investing since summer of 18. That’s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>gzv1g5o</td>\n",
       "      <td>2021-05-29 11:31:29</td>\n",
       "      <td>t1_gzv1g5o</td>\n",
       "      <td>t1_gzv12o4</td>\n",
       "      <td>stocks</td>\n",
       "      <td>I'm not investing due to their ethics. They kn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>gzv1kwz</td>\n",
       "      <td>2021-05-29 11:33:23</td>\n",
       "      <td>t1_gzv1kwz</td>\n",
       "      <td>t3_nnl9c5</td>\n",
       "      <td>stocks</td>\n",
       "      <td>I will also be looking to trade it around 200....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>gzv1lgo</td>\n",
       "      <td>2021-05-29 11:33:35</td>\n",
       "      <td>t1_gzv1lgo</td>\n",
       "      <td>t1_gzu51sv</td>\n",
       "      <td>stocks</td>\n",
       "      <td>This right here. Proven over the last 9 weeks ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 comment_id              created    fullname   parent_id  \\\n",
       "0              0    gzsuulr  2021-05-28 20:39:11  t1_gzsuulr  t1_gzpc2r8   \n",
       "1              1        NaN  2021-05-28 20:39:14   t3_nn724o         NaN   \n",
       "2              2    gzsuwbn  2021-05-28 20:39:34  t1_gzsuwbn   t3_nn5tcm   \n",
       "3              3    gzsuyhy  2021-05-28 20:40:03  t1_gzsuyhy  t1_gzrz5t7   \n",
       "4              4    gzsuz0n  2021-05-28 20:40:10  t1_gzsuz0n  t1_gzssh7x   \n",
       "...          ...        ...                  ...         ...         ...   \n",
       "996          996    gzv1em4  2021-05-29 11:30:53  t1_gzv1em4   t3_nn724o   \n",
       "997          997    gzv1epl  2021-05-29 11:30:55  t1_gzv1epl  t1_gzu5fmn   \n",
       "998          998    gzv1g5o  2021-05-29 11:31:29  t1_gzv1g5o  t1_gzv12o4   \n",
       "999          999    gzv1kwz  2021-05-29 11:33:23  t1_gzv1kwz   t3_nnl9c5   \n",
       "1000        1000    gzv1lgo  2021-05-29 11:33:35  t1_gzv1lgo  t1_gzu51sv   \n",
       "\n",
       "     subreddit                                               text post_id  \\\n",
       "0       stocks  They denied me when I asked for a BRK discount :(     NaN   \n",
       "1       stocks  I'm sure you know those two has stock splits. ...  nn724o   \n",
       "2       stocks                                               Mom?     NaN   \n",
       "3       stocks  I have medical conditions that they are workin...     NaN   \n",
       "4       stocks  It's daunting at first. A quick list of topics...     NaN   \n",
       "...        ...                                                ...     ...   \n",
       "996     stocks  You missed the big run up on TTD after its ear...     NaN   \n",
       "997     stocks  I’ve been investing since summer of 18. That’s...     NaN   \n",
       "998     stocks  I'm not investing due to their ethics. They kn...     NaN   \n",
       "999     stocks  I will also be looking to trade it around 200....     NaN   \n",
       "1000    stocks  This right here. Proven over the last 9 weeks ...     NaN   \n",
       "\n",
       "                                title  \n",
       "0                                 NaN  \n",
       "1     TTD or NVDA which one is better  \n",
       "2                                 NaN  \n",
       "3                                 NaN  \n",
       "4                                 NaN  \n",
       "...                               ...  \n",
       "996                               NaN  \n",
       "997                               NaN  \n",
       "998                               NaN  \n",
       "999                               NaN  \n",
       "1000                              NaN  \n",
       "\n",
       "[1001 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_clean = pd.read_csv('data_files/wallstreetbets_21.csv', lineterminator='\\n')\n",
    "data = pd.read_csv('../Data_Collecting/data_files/Stocks_34.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absent-burner",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>created</th>\n",
       "      <th>fullname</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gylrunc</td>\n",
       "      <td>2021-05-18 19:10:20</td>\n",
       "      <td>t1_gylrunc</td>\n",
       "      <td>t3_nf66zs</td>\n",
       "      <td>stocks</td>\n",
       "      <td>And it’s all going green again</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>gylrwn7</td>\n",
       "      <td>2021-05-18 19:10:43</td>\n",
       "      <td>t1_gylrwn7</td>\n",
       "      <td>t1_gylh7q1</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Uh no. I already signed up forv$10K add on to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>gylrzsa</td>\n",
       "      <td>2021-05-18 19:11:19</td>\n",
       "      <td>t1_gylrzsa</td>\n",
       "      <td>t1_gylhiir</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Billionaire investors aren’t actually any smar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>gylrzv8</td>\n",
       "      <td>2021-05-18 19:11:20</td>\n",
       "      <td>t1_gylrzv8</td>\n",
       "      <td>t1_gylhiir</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Well, he’s a monkey, so...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>gyls2uk</td>\n",
       "      <td>2021-05-18 19:11:54</td>\n",
       "      <td>t1_gyls2uk</td>\n",
       "      <td>t3_nfj008</td>\n",
       "      <td>stocks</td>\n",
       "      <td>There are only 2 rules i follow in investing:\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>gyn37rf</td>\n",
       "      <td>2021-05-19 01:06:16</td>\n",
       "      <td>t1_gyn37rf</td>\n",
       "      <td>t1_gyn2tuk</td>\n",
       "      <td>stocks</td>\n",
       "      <td>You said they wouldn’t do shit with a small am...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>gyn3cx3</td>\n",
       "      <td>2021-05-19 01:07:29</td>\n",
       "      <td>t1_gyn3cx3</td>\n",
       "      <td>t3_nfrmgh</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Bruh let me tell you the bags I’m still holdin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>gyn3d63</td>\n",
       "      <td>2021-05-19 01:07:32</td>\n",
       "      <td>t1_gyn3d63</td>\n",
       "      <td>t1_gymwjcc</td>\n",
       "      <td>stocks</td>\n",
       "      <td>I actually remember people showing higher buy ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>gyn3e48</td>\n",
       "      <td>2021-05-19 01:07:46</td>\n",
       "      <td>t1_gyn3e48</td>\n",
       "      <td>t3_nfrmgh</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Evfm $2.00 may 21 call. Tanked 34% ATH yesterd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>gyn3lnl</td>\n",
       "      <td>2021-05-19 01:09:31</td>\n",
       "      <td>t1_gyn3lnl</td>\n",
       "      <td>t3_nfrmgh</td>\n",
       "      <td>stocks</td>\n",
       "      <td>Rmo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 comment_id              created    fullname   parent_id  \\\n",
       "0              0    gylrunc  2021-05-18 19:10:20  t1_gylrunc   t3_nf66zs   \n",
       "1              1    gylrwn7  2021-05-18 19:10:43  t1_gylrwn7  t1_gylh7q1   \n",
       "2              2    gylrzsa  2021-05-18 19:11:19  t1_gylrzsa  t1_gylhiir   \n",
       "3              3    gylrzv8  2021-05-18 19:11:20  t1_gylrzv8  t1_gylhiir   \n",
       "4              4    gyls2uk  2021-05-18 19:11:54  t1_gyls2uk   t3_nfj008   \n",
       "...          ...        ...                  ...         ...         ...   \n",
       "996          996    gyn37rf  2021-05-19 01:06:16  t1_gyn37rf  t1_gyn2tuk   \n",
       "997          997    gyn3cx3  2021-05-19 01:07:29  t1_gyn3cx3   t3_nfrmgh   \n",
       "998          998    gyn3d63  2021-05-19 01:07:32  t1_gyn3d63  t1_gymwjcc   \n",
       "999          999    gyn3e48  2021-05-19 01:07:46  t1_gyn3e48   t3_nfrmgh   \n",
       "1000        1000    gyn3lnl  2021-05-19 01:09:31  t1_gyn3lnl   t3_nfrmgh   \n",
       "\n",
       "     subreddit                                               text post_id  \\\n",
       "0       stocks                     And it’s all going green again     NaN   \n",
       "1       stocks  Uh no. I already signed up forv$10K add on to ...     NaN   \n",
       "2       stocks  Billionaire investors aren’t actually any smar...     NaN   \n",
       "3       stocks                         Well, he’s a monkey, so...     NaN   \n",
       "4       stocks  There are only 2 rules i follow in investing:\\...     NaN   \n",
       "...        ...                                                ...     ...   \n",
       "996     stocks  You said they wouldn’t do shit with a small am...     NaN   \n",
       "997     stocks  Bruh let me tell you the bags I’m still holdin...     NaN   \n",
       "998     stocks  I actually remember people showing higher buy ...     NaN   \n",
       "999     stocks  Evfm $2.00 may 21 call. Tanked 34% ATH yesterd...     NaN   \n",
       "1000    stocks                                                Rmo     NaN   \n",
       "\n",
       "     title  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  \n",
       "...    ...  \n",
       "996    NaN  \n",
       "997    NaN  \n",
       "998    NaN  \n",
       "999    NaN  \n",
       "1000   NaN  \n",
       "\n",
       "[1001 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('../Data_Collecting/data_files/Stocks_3.csv')\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "unlimited-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_filenames = []\n",
    "old_filenames.append('..Data_Collecting/data_files/wallstreetbets_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "popular-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_filenames.append('..Data_Collecting/data_files/wallstreetbets_44.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "periodic-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "submissions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foster-header",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1dcaacf13e01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mcomme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/sample_data/comment_sample'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "appointed-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for c in comments:\n",
    "    c.to_csv('/data_files/comment_sample'+str(counter)+'.csv')\n",
    "    counter +=1\n",
    "for s in submissions:\n",
    "    s.to_csv('data_files/submission_sample'+str(counter)+'.csv')\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-reservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ../Data_Collecting/data_files/Stocks_3.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_52.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_46.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_5.csv\n",
      "* ../Data_Collecting/data_files/Stocks_15.csv\n",
      "* ../Data_Collecting/data_files/Stocks_29.csv\n",
      "* ../Data_Collecting/data_files/Stocks_28.csv\n",
      "* ../Data_Collecting/data_files/Stocks_14.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_4.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_47.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_53.csv\n",
      "* ../Data_Collecting/data_files/Stocks_2.csv\n",
      "* ../Data_Collecting/data_files/Stocks_0.csv\n",
      "* ../Data_Collecting/data_files/pennystocks_11.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_45.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_51.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_6.csv\n",
      "* ../Data_Collecting/data_files/Stocks_16.csv\n",
      "* ../Data_Collecting/data_files/Stocks_17.csv\n",
      "* ../Data_Collecting/data_files/wallstreetbets_7.csv\n",
      "* ../Data_Collecting/data_files/pennystocks_10.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_1.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_5.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_40.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_54.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_3.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_10.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_13.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_12.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_11.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_2.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_55.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_41.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_4.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_6.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_43.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_0.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_13.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_10.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Investing_8.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_11.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Investing_9.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_12.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_1.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_42.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Stocks_7.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_0.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_31.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_25.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_19.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Investing_19.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_18.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Investing_18.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_24.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_30.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_1.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/pennystocks_9.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_3.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_26.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_32.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_33.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_27.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_2.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/pennystocks_8.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_6.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/algotrading_2.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_23.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_37.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_36.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_22.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/algotrading_3.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_7.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_5.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/algotrading_1.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_34.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Investing_20.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_20.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Investing_21.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_21.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_35.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/algotrading_0.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_4.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/pennystocks_3.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/stockMarket_9.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/Investing_10.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_10.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* ../Data_Collecting/data_files/wallstreetbets_38.csv\n",
      "   * Transforming data\n"
     ]
    }
   ],
   "source": [
    "# read all csv files\n",
    "for file_name in glob.glob('../Data_Collecting/data_files/'+'*.csv'):\n",
    "    print('* '+file_name)\n",
    "    if file_name not in old_filenames:\n",
    "        data = pd.read_csv(file_name,lineterminator='\\n')\n",
    "        # Transform data\n",
    "        print('   * Transforming data')\n",
    "        dfs = transform_reddit_data(data)\n",
    "        df_submissions = dfs[0]\n",
    "        df_comments = dfs[1]\n",
    "        # Persist submissions and comments \n",
    "        print('   * Persisting submissions')\n",
    "       # insert_submissions(df_submissions)\n",
    "        submissions.append(df_submissions)\n",
    "        print('   * Persisting comments')\n",
    "        #insert_comments(df_comments)\n",
    "        comments.append(df_comments)\n",
    "        old_filenames.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-breakdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data_Collecting/data_files/Stocks_3.csv')\n",
    "df_submissions = data.drop(['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title'],axis=1)\n",
    "df_submissions['created'] = df_submissions['created'].apply(datetime_to_date)\n",
    "df_submissions = df_submissions.dropna()\n",
    "df_submissions.rename(columns = {'post_id':'id'}, inplace = True)\n",
    "# Apply NER\n",
    "df_submissions['Organizations'] = df_submissions['text'].apply(get_entities).apply(clean_orgs)\n",
    "# Apply Sentiment Analysis\n",
    "df_submissions['Sentiment'] = df_submissions['text'].apply(predictSentiment)\n",
    "insert_submissions(df_submissions)\n",
    "df_comments = data.drop(['Unnamed: 0','post_id', 'fullname', 'title'],axis=1)\n",
    "df_comments['created'] = df_comments['created'].apply(datetime_to_date)\n",
    "df_comments = df_comments.dropna()\n",
    "# Apply NER\n",
    "df_comments['Organizations'] = df_comments['text'].apply(get_entities).apply(clean_orgs)\n",
    "# Apply Sentiment Analysis\n",
    "df_comments['Sentiment'] = df_comments['text'].apply(predictSentiment)\n",
    "insert_comments(df_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments[5:55]\n",
    "#df_comments.to_csv('comments_sample_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-russell",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-parts",
   "metadata": {},
   "source": [
    "## Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "inside-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list with submissions dataframe and comments dataframe\n",
    "def transform_reddit_data(data: pd.DataFrame):\n",
    "        dfs = []\n",
    "        # submissions\n",
    "        #candidates =['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title']\n",
    "        #df_submissions = data.drop([x for x in candidates if x in data.columns], axis=1)\n",
    "        df_submissions = data.drop(['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title'],axis=1)\n",
    "        df_submissions['created'] = df_submissions['created'].apply(datetime_to_date)\n",
    "        df_submissions = df_submissions.dropna()\n",
    "        df_submissions.rename(columns = {'post_id':'id'}, inplace = True)\n",
    "        df_submissions['Organizations'] = df_submissions['text'].apply(get_entities).apply(clean_orgs)\n",
    "        df_submissions['Sentiment'] = df_submissions['text'].apply(predictSentiment)\n",
    "        dfs.append(df_submissions)\n",
    "        # comments\n",
    "        #candidates =['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title']\n",
    "        #df_comments = data.drop([x for x in candidates if x in data.columns], axis=1)\n",
    "        df_comments = data.drop(['Unnamed: 0','post_id', 'fullname', 'title'],axis=1)\n",
    "        df_comments['created'] = df_comments['created'].apply(datetime_to_date)\n",
    "        df_comments = df_comments.dropna()\n",
    "        df_comments['Organizations'] = df_comments['text'].apply(get_entities).apply(clean_orgs)\n",
    "        df_comments['Sentiment'] = df_comments['text'].apply(predictSentiment)\n",
    "        dfs.append(df_comments)\n",
    "        \n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "technical-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes utc timestamp to datetime.date\n",
    "def datetime_to_date(timestamp):\n",
    "    return pd.to_datetime(timestamp).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mechanical-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mentions that we are interested in\n",
    "selected_orgs =  ['HKG', 'Alibaba','AMC', 'Palantir Technologies', 'PLTR', 'FORD', 'Lordstown Motors', 'RIDE', 'Virgin Galactic', 'SPCE', 'AI', 'C3.AI', 'TSLA', 'GE', 'GME', 'AAPL', 'Tesla', 'Apple', 'General Electric', 'GE', 'NOK', 'Nokia']\n",
    "orgs_dict = {\"Alibaba\":\"HKG\",\"AMC\":\"AMC\",\"Palantir Technologies\":\"PLTR\",\"FORD\":\"FORD\", \"Lordstown Motors\":\"RIDE\",\"Virgin Galactic\":\"SPCE\",\"c3.AI\":\"AI\",\"Tesla\":\"TSLA\", \"General Electric\":\"GE\",\"Apple\":\"AAPL\",\"GameStop\":\"GME\",\"Gamestop\":\"GME\", \"Nokia\":\"NOK\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-anatomy",
   "metadata": {},
   "source": [
    "Clean orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "downtown-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning a list of mentioned tickers\n",
    "def clean_orgs(organizations):\n",
    "    orgs = []\n",
    "    for org in organizations:\n",
    "        if org in selected_orgs:\n",
    "            if org in orgs_dict:\n",
    "                org = orgs_dict[org]\n",
    "                orgs.append(org)\n",
    "            else: \n",
    "                orgs.append(org)\n",
    "    for org in orgs:\n",
    "        o = set(orgs)\n",
    "        orgs = list(o)\n",
    "    return orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bound-reliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPCE']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_orgs(['Virgin Galactic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-pregnancy",
   "metadata": {},
   "source": [
    "## A Look at the mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "contrary-contractor",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creates a list of mentionend entities\n",
    "\n",
    "#data['Organizations'] = data['text'].apply(get_entities)\n",
    "#orgs = data['Organizations'].to_list()\n",
    "#orgs_flat = [org for sublist in orgs for org in sublist] # Pulls out entities from the nested lists in orgs => new flat list\n",
    "# Print 20 most mentions ORGs\n",
    "#from collections import Counter\n",
    "#org_freq = Counter(orgs_flat)\n",
    "#org_freq.most_common(20)                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-forum",
   "metadata": {},
   "source": [
    "## Transform and save the data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submissionsransforming into lists of submissions and comments\n",
    "dfs = transform_reddit_data(data)\n",
    "submissions = dfs[0]\n",
    "comments = dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-season",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SUBMISSIONS subset\n",
    "#print(len(submissions))\n",
    "df = submissions[1:500]\n",
    "insert_submissions(df)\n",
    "#COMMENTS subset\n",
    "#print(len(comments))\n",
    "#df_c = comments[500:5500]\n",
    "#\n",
    "#insert_comments(df_c)\n",
    "\n",
    "#insert_c(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://localhost:5050/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "innovative-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_submissions(dataframe: pd.DataFrame):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        #create submission nodes\n",
    "        url2 = 'submission/'+row['subreddit']+'/'+row['created'].strftime('%Y-%m-%d')+'/'+row['id']\n",
    "        response = requests.post(url+url2)\n",
    "        if not response.status_code == 200:\n",
    "            print ('submission id '+ row['id'] + ' failed insertion')\n",
    "        # create mentions relationships\n",
    "        for org in row['Organizations']:\n",
    "            url3 = 'connection/mentions/submission/'+row['id']+'/'+ org + '/' + row['created'].strftime('%Y-%m-%d')\n",
    "            response = requests.post(url+url3)\n",
    "            if not response.status_code == 200:\n",
    "                print ('mentions between '+ row['id'] + ' and ' + org + ' failed creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "integral-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_comments(dataframe: pd.DataFrame):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        #create comment nodes\n",
    "        url2 = 'comment/'+row['subreddit']+'/'+row['created'].strftime('%m-%d-%Y')+'/'+row['comment_id']+'/'+row['parent_id']\n",
    "        response = requests.post(url+url2)\n",
    "        if not response.status_code == 200:\n",
    "            print ('comment id '+ row['comment_id'] + ' failed insertion')\n",
    "            \n",
    "        # create mentions relationships\n",
    "        for org in row['Organizations']:\n",
    "            url3 = 'connection/mentions/comment/'+row['comment_id']+'/'+ org +'/'+ row['created'].strftime('%m-%d-%Y')\n",
    "            response = requests.post(url+url3)\n",
    "            if not response.status_code == 200:\n",
    "                print ('mention between '+ row['comment_id'] + ' and ' + org + ' failed creation')\n",
    "    \n",
    "           \n",
    "        # create replies relationships\n",
    "        row['parent_id'] = pd.Series(row['parent_id'], dtype=\"string\") #turning parent_id from type Series to String\n",
    "        for parent_id in row['parent_id']:\n",
    "            p_id = parent_id[3:]\n",
    "            if parent_id[:2] == 't1':\n",
    "                url_reply = 'connection/replies/comment/' + row['comment_id'] + '/' + p_id\n",
    "            elif parent_id[:2] == 't3':\n",
    "                url_reply = 'connection/replies/submission/' + row['comment_id'] + '/' + p_id\n",
    "            response = requests.post(url+url_reply)\n",
    "            if not response.status_code == 200:\n",
    "                print ('replies relationship between '+ row['comment_id'] + ' and ' + p_id + ' failed creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-specification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-going",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-calibration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-montreal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-czech",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-austria",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
