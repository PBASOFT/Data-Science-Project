{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import string\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is the first. this is the second. this is the third\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is the first. this is the second. this is the third'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "is\n",
      "the\n",
      "first\n",
      ".\n",
      "this\n",
      "is\n",
      "the\n",
      "second\n",
      ".\n",
      "this\n",
      "is\n",
      "the\n",
      "third\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the first.\n",
      "this is the second.\n",
      "this is the third\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "second\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.is_stop == False:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"run running runs runner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "running run\n",
      "runs run\n",
      "runner runner\n"
     ]
    }
   ],
   "source": [
    "for lem in doc:\n",
    "    print(lem.text, lem.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yelp = pd.read_csv('../trainingDataForSentiment/yelp_labelled.txt', sep='\\t', header = None)\n",
    "data_amazon = pd.read_csv('../trainingDataForSentiment/amazon_cells_labelled.txt', sep='\\t', header = None)\n",
    "data_imdb = pd.read_csv('../trainingDataForSentiment/imdb_labelled.txt', sep='\\t', header = None)\n",
    "\n",
    "reddit_test = pd.read_csv('../trainingDataForSentiment/reddit_test.txt', sep='\\t', header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = ['Review', 'Sentiment']\n",
    "data_yelp.columns = columns_name\n",
    "data_amazon.columns = columns_name\n",
    "data_imdb.columns = columns_name\n",
    "reddit_test.columns = columns_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 2)\n",
      "(748, 2)\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data_yelp.shape)\n",
    "print(data_amazon.shape)\n",
    "print(data_imdb.shape)\n",
    "print(reddit_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                  Review  Sentiment\n",
      "2679  I do not know if this was Emilio Estevez's dir...          1\n",
      "2215       The camera really likes her in this movie.            1\n",
      "64                  We are so glad we found this place.          1\n",
      "129   Definitely worth venturing off the strip for t...          1\n",
      "2548  In fact, I liked it better than Interview With...          1\n",
      "...                                                 ...        ...\n",
      "6                 Honeslty it didn't taste THAT fresh.)          0\n",
      "2328  His losing his marbles so early in the proceed...          0\n",
      "1149  After 3 months, screen just went black all of ...          0\n",
      "1413  We received a WHITE colored battery that goes ...          0\n",
      "717   Tonight I had the Elk Filet special...and it s...          0\n",
      "\n",
      "[2748 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "data = data_yelp.append([data_amazon, data_imdb], ignore_index=True)\n",
    "data = shuffle(data)\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1386\n",
       "0    1362\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review       0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower()\n",
    "        tokens.append(temp)\n",
    "    cleaned_tokens = []    \n",
    "    for token in tokens:\n",
    "        if token not in stopwords and token not in punct:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'like', 'video']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_cleaning(\"Hello how are you. Like this video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=text_data_cleaning)\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reddit = reddit_test['Review']\n",
    "Y_reddit = reddit_test['Sentiment']\n",
    "\n",
    "X = data['Review']\n",
    "y = data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x7fe5be612790>)),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "type(reddit_test)\n",
    "#type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77       271\n",
      "           1       0.77      0.79      0.78       279\n",
      "\n",
      "    accuracy                           0.77       550\n",
      "   macro avg       0.77      0.77      0.77       550\n",
      "weighted avg       0.77      0.77      0.77       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[205,  66],\n",
       "       [ 59, 220]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([\"What do you guys Think on NOK will it or will it not. I think it will.\",\"I LIKE NOK pt 2.\",\"Be careful. Air Canada filed for bankruptcy the last recession. New management was structured with the same company name, but the existing shareholders were wiped out.\",\"Remember when hertz filed for bankruptcy and you missed out on all those gainz? This is your shot. \",\"Do not invest in the stock of a bankrupt company. Bankruptcy is an indication of insolvency, meaning that the fair value of the liabilities (both recorded and unrecorded) exceed the fair value of the assets. It is very rare (not never, but very rare) that there are sufficient funds to fully compensate the secured and unsecured creditors, nevermind equity.\",\"How did you arrive at 20% clean energy ETFs? I personally wouldn't allocate more than 1- 2% of my portfolio of those ETFs if I was rebalancing.\",\"Yes, the god of losing 1/3rd of your money in 3 months. Here's to all the poor saps that bought into ARKK at the highs.\",\"RIP my portfolio of G and K\",\"Roku was a steal today. People thought it would pull a Netflix despite ad spending (Goog/FB) and HBO growth hitting records.\",\"yay penny stocks!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSentiment(textToPredict):\n",
    "    return clf.predict([textToPredict])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
