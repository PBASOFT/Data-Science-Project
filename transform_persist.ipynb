{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipynb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "square-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.NER import get_entities\n",
    "import pandas as pd \n",
    "import datetime\n",
    "import requests\n",
    "from collections import Counter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-completion",
   "metadata": {},
   "source": [
    "### Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "retained-guide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>created</th>\n",
       "      <th>fullname</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gz117qx</td>\n",
       "      <td>2021-05-22 05:37:10</td>\n",
       "      <td>t1_gz117qx</td>\n",
       "      <td>t1_gz10ocl</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Boohoo, go read a children‚Äôs bed time story wh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>gz1187q</td>\n",
       "      <td>2021-05-22 05:37:20</td>\n",
       "      <td>t1_gz1187q</td>\n",
       "      <td>t1_gz10shp</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>!GUH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>gz11882</td>\n",
       "      <td>2021-05-22 05:37:21</td>\n",
       "      <td>t1_gz11882</td>\n",
       "      <td>t1_gz1187q</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>#[GUH](https://www.reddit.com/r/wallstreetbets...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>gz11897</td>\n",
       "      <td>2021-05-22 05:37:21</td>\n",
       "      <td>t1_gz11897</td>\n",
       "      <td>t1_gz10wyw</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>In all fairness the streets are where I get my...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>gz1189c</td>\n",
       "      <td>2021-05-22 05:37:21</td>\n",
       "      <td>t1_gz1189c</td>\n",
       "      <td>t1_gz10u2g</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Why</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>gz2wmte</td>\n",
       "      <td>2021-05-22 18:29:45</td>\n",
       "      <td>t1_gz2wmte</td>\n",
       "      <td>t1_gz2wjg9</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Making money here is like being paid to be the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>gz2wn05</td>\n",
       "      <td>2021-05-22 18:29:48</td>\n",
       "      <td>t1_gz2wn05</td>\n",
       "      <td>t3_ni1fw5</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Hype is the most interesting of all fundamenta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>gz2wn4h</td>\n",
       "      <td>2021-05-22 18:29:49</td>\n",
       "      <td>t1_gz2wn4h</td>\n",
       "      <td>t3_niory4</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Oh wow they went up and down... again....\\n\\nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>gz2wn6r</td>\n",
       "      <td>2021-05-22 18:29:50</td>\n",
       "      <td>t1_gz2wn6r</td>\n",
       "      <td>t3_nimruw</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>I always loved the stock long term, we are loo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>10000</td>\n",
       "      <td>gz2wn95</td>\n",
       "      <td>2021-05-22 18:29:51</td>\n",
       "      <td>t1_gz2wn95</td>\n",
       "      <td>t1_gz2d986</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>I decided guys I will work more to become a wh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10001 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 comment_id              created    fullname   parent_id  \\\n",
       "0               0    gz117qx  2021-05-22 05:37:10  t1_gz117qx  t1_gz10ocl   \n",
       "1               1    gz1187q  2021-05-22 05:37:20  t1_gz1187q  t1_gz10shp   \n",
       "2               2    gz11882  2021-05-22 05:37:21  t1_gz11882  t1_gz1187q   \n",
       "3               3    gz11897  2021-05-22 05:37:21  t1_gz11897  t1_gz10wyw   \n",
       "4               4    gz1189c  2021-05-22 05:37:21  t1_gz1189c  t1_gz10u2g   \n",
       "...           ...        ...                  ...         ...         ...   \n",
       "9996         9996    gz2wmte  2021-05-22 18:29:45  t1_gz2wmte  t1_gz2wjg9   \n",
       "9997         9997    gz2wn05  2021-05-22 18:29:48  t1_gz2wn05   t3_ni1fw5   \n",
       "9998         9998    gz2wn4h  2021-05-22 18:29:49  t1_gz2wn4h   t3_niory4   \n",
       "9999         9999    gz2wn6r  2021-05-22 18:29:50  t1_gz2wn6r   t3_nimruw   \n",
       "10000       10000    gz2wn95  2021-05-22 18:29:51  t1_gz2wn95  t1_gz2d986   \n",
       "\n",
       "            subreddit                                               text  \\\n",
       "0      wallstreetbets  Boohoo, go read a children‚Äôs bed time story wh...   \n",
       "1      wallstreetbets                                               !GUH   \n",
       "2      wallstreetbets  #[GUH](https://www.reddit.com/r/wallstreetbets...   \n",
       "3      wallstreetbets  In all fairness the streets are where I get my...   \n",
       "4      wallstreetbets                                                Why   \n",
       "...               ...                                                ...   \n",
       "9996   wallstreetbets  Making money here is like being paid to be the...   \n",
       "9997   wallstreetbets  Hype is the most interesting of all fundamenta...   \n",
       "9998   wallstreetbets  Oh wow they went up and down... again....\\n\\nS...   \n",
       "9999   wallstreetbets  I always loved the stock long term, we are loo...   \n",
       "10000  wallstreetbets  I decided guys I will work more to become a wh...   \n",
       "\n",
       "      post_id title  \n",
       "0         NaN   NaN  \n",
       "1         NaN   NaN  \n",
       "2         NaN   NaN  \n",
       "3         NaN   NaN  \n",
       "4         NaN   NaN  \n",
       "...       ...   ...  \n",
       "9996      NaN   NaN  \n",
       "9997      NaN   NaN  \n",
       "9998      NaN   NaN  \n",
       "9999      NaN   NaN  \n",
       "10000     NaN   NaN  \n",
       "\n",
       "[10001 rows x 9 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_clean = pd.read_csv('data_files/wallstreetbets_21.csv', lineterminator='\\n')\n",
    "#data = pd.read_csv('data_files/wallstreetbets_21.csv')\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "available-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_filenames = []\n",
    "old_filenames.append('wallstreetbets_44.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "secure-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* DATA/reddit/Stocks_3.csv\n",
      "* DATA/reddit/Stocks_3.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_52.csv\n",
      "* DATA/reddit/wallstreetbets_52.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_46.csv\n",
      "* DATA/reddit/wallstreetbets_46.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_5.csv\n",
      "* DATA/reddit/wallstreetbets_5.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/Stocks_15.csv\n",
      "* DATA/reddit/Stocks_15.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/Stocks_29.csv\n",
      "* DATA/reddit/Stocks_29.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/Stocks_28.csv\n",
      "* DATA/reddit/Stocks_28.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/Stocks_14.csv\n",
      "* DATA/reddit/Stocks_14.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_4.csv\n",
      "* DATA/reddit/wallstreetbets_4.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_47.csv\n",
      "* DATA/reddit/wallstreetbets_47.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_53.csv\n",
      "* DATA/reddit/wallstreetbets_53.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/Stocks_2.csv\n",
      "* DATA/reddit/Stocks_2.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/Stocks_0.csv\n",
      "* DATA/reddit/Stocks_0.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/pennystocks_11.csv\n",
      "* DATA/reddit/pennystocks_11.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_45.csv\n",
      "* DATA/reddit/wallstreetbets_45.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_51.csv\n",
      "* DATA/reddit/wallstreetbets_51.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_6.csv\n",
      "* DATA/reddit/wallstreetbets_6.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/Stocks_16.csv\n",
      "* DATA/reddit/Stocks_16.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/Stocks_17.csv\n",
      "* DATA/reddit/Stocks_17.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_7.csv\n",
      "* DATA/reddit/wallstreetbets_7.csv\n",
      "   * Transforming data\n",
      "   * Persisting submissions\n",
      "   * Persisting comments\n",
      "* DATA/reddit/wallstreetbets_50.csv\n",
      "* DATA/reddit/wallstreetbets_50.csv\n",
      "   * Transforming data\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['post_id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-d896631dd898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Transform data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'   * Transforming data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_reddit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdf_submissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdf_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-7867b5c6de33>\u001b[0m in \u001b[0;36mtransform_reddit_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#candidates =['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#df_comments = data.drop([x for x in candidates if x in data.columns], axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdf_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fullname'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'post_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdf_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime_to_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdf_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_comments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4303\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4304\u001b[0m         \"\"\"\n\u001b[0;32m-> 4305\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4306\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4307\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4152\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4185\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4187\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4188\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5591\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['post_id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# read all csv files\n",
    "for file_name in glob.glob('DATA/reddit/'+'*.csv'):\n",
    "    print('* '+file_name)\n",
    "    if file_name not in old_filenames:\n",
    "        data = pd.read_csv(file_name,lineterminator='\\n')\n",
    "        # Transform data\n",
    "        print('   * Transforming data')\n",
    "        dfs = transform_reddit_data(data)\n",
    "        df_submissions = dfs[0]\n",
    "        df_comments = dfs[1]\n",
    "        # Persist submissions and comments \n",
    "        print('   * Persisting submissions')\n",
    "        insert_submissions(df_submissions)\n",
    "        print('   * Persisting comments')\n",
    "        insert_comments(df_comments)\n",
    "        old_filenames.append(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-sector",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-stanley",
   "metadata": {},
   "source": [
    "## Transforming the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "controlled-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list with submissions dataframe and comments dataframe\n",
    "def transform_reddit_data(data: pd.DataFrame):\n",
    "        dfs = []\n",
    "        # submissions\n",
    "        #candidates =['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title']\n",
    "        #df_submissions = data.drop([x for x in candidates if x in data.columns], axis=1)\n",
    "        df_submissions = data.drop(['Unnamed: 0', 'fullname'],axis=1)\n",
    "        df_submissions['created'] = df_submissions['created'].apply(datetime_to_date)\n",
    "        df_submissions = df_submissions.dropna()\n",
    "        df_submissions.rename(columns = {'post_id':'id'}, inplace = True)\n",
    "        df_submissions['Organizations'] = df_submissions['text'].apply(get_entities).apply(clean_orgs)\n",
    "        dfs.append(df_submissions)\n",
    "        # comments\n",
    "        #candidates =['Unnamed: 0','comment_id', 'fullname', 'parent_id', 'title']\n",
    "        #df_comments = data.drop([x for x in candidates if x in data.columns], axis=1)\n",
    "        df_comments = data.drop(['Unnamed: 0', 'fullname', 'post_id'],axis=1)\n",
    "        df_comments['created'] = df_comments['created'].apply(datetime_to_date)\n",
    "        df_comments = df_comments.dropna()\n",
    "        df_comments['Organizations'] = df_comments['text'].apply(get_entities).apply(clean_orgs)\n",
    "        dfs.append(df_comments)\n",
    "        \n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "innocent-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes utc timestamp to datetime.date\n",
    "def datetime_to_date(timestamp):\n",
    "    return pd.to_datetime(timestamp).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "white-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mentions that we are interested in\n",
    "selected_orgs =  ['HKG', 'Alibaba','AMC', 'Palantir Technologies', 'PLTR', 'FORD', 'Lordstown Motors', 'RIDE', 'Virgin Galactic', 'SPCE', 'AI', 'C3.AI', 'TSLA', 'GE', 'GME', 'AAPL', 'Tesla', 'Apple', 'General Electric', 'GE', 'NOK', 'Nokia']\n",
    "orgs_dict = {\"Alibaba\":\"HKG\",\"AMC\":\"AMC\",\"Palantir Technologies\":\"PLTR\",\"FORD\":\"FORD\", \"Lordstown Motors\":\"RIDE\",\"Virgin Galactic\":\"SPCE\",\"c3.AI\":\"AI\",\"Tesla\":\"TSLA\", \"General Electric\":\"GE\",\"Apple\":\"AAPL\",\"GameStop\":\"GME\",\"Gamestop\":\"GME\", \"Nokia\":\"NOK\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spare-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning a list of mentioned tickers\n",
    "def clean_orgs(organizations):\n",
    "    orgs = []\n",
    "    for org in organizations:\n",
    "        if org in selected_orgs:\n",
    "            if org in orgs_dict:\n",
    "                org = orgs_dict[org]\n",
    "                orgs.append(org)\n",
    "            else: \n",
    "                orgs.append(org)\n",
    "    for org in orgs:\n",
    "        o = set(orgs)\n",
    "        orgs = list(o)\n",
    "    return orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "racial-asian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPCE']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_orgs(['Virgin Galactic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-journalist",
   "metadata": {},
   "source": [
    "## A Look at the mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "desperate-feelings",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-452a17db1e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creates a list of mentionend entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Organizations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_entities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0morgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Organizations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0morgs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0morg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morgs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0morg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Pulls out entities from the nested lists in orgs => new flat list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print 20 most mentions ORGs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Creates a list of mentionend entities\n",
    "\n",
    "#data['Organizations'] = data['text'].apply(get_entities)\n",
    "#orgs = data['Organizations'].to_list()\n",
    "#orgs_flat = [org for sublist in orgs for org in sublist] # Pulls out entities from the nested lists in orgs => new flat list\n",
    "# Print 20 most mentions ORGs\n",
    "#from collections import Counter\n",
    "#org_freq = Counter(orgs_flat)\n",
    "#org_freq.most_common(20)                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-chicken",
   "metadata": {},
   "source": [
    "## Transform and save the data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "circular-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submissionsransforming into lists of submissions and comments\n",
    "dfs = transform_reddit_data(data)\n",
    "submissions = dfs[0]\n",
    "comments = dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "atlantic-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>created</th>\n",
       "      <th>fullname</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gzo4vsv</td>\n",
       "      <td>2021-05-27 19:02:58</td>\n",
       "      <td>t1_gzo4vsv</td>\n",
       "      <td>t1_gzo3wbw</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Yes. Is it smart? Debatable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>gzo4vv6</td>\n",
       "      <td>2021-05-27 19:02:58</td>\n",
       "      <td>t1_gzo4vv6</td>\n",
       "      <td>t1_gzo4lej</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>put a majority of my account into this based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>gzo4vvp</td>\n",
       "      <td>2021-05-27 19:02:59</td>\n",
       "      <td>t1_gzo4vvp</td>\n",
       "      <td>t1_gzo4r94</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>We are in this togheter brotherü¶ç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>gzo4vwz</td>\n",
       "      <td>2021-05-27 19:02:59</td>\n",
       "      <td>t1_gzo4vwz</td>\n",
       "      <td>t3_nm414o</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>CLOV!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>gzo4vyx</td>\n",
       "      <td>2021-05-27 19:03:00</td>\n",
       "      <td>t1_gzo4vyx</td>\n",
       "      <td>t3_nm414o</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Now do for PLTR what you did for AMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>gzoh41e</td>\n",
       "      <td>2021-05-27 20:30:39</td>\n",
       "      <td>t1_gzoh41e</td>\n",
       "      <td>t3_nmg57b</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>SPCE baby girls.  I need some hooker money for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>gzoh45k</td>\n",
       "      <td>2021-05-27 20:30:41</td>\n",
       "      <td>t1_gzoh45k</td>\n",
       "      <td>t1_gzogdpm</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>I thought about that today too. It has differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>gzoh461</td>\n",
       "      <td>2021-05-27 20:30:41</td>\n",
       "      <td>t1_gzoh461</td>\n",
       "      <td>t1_gzogoih</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>It's been in the $10 range for months tho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>gzoh470</td>\n",
       "      <td>2021-05-27 20:30:41</td>\n",
       "      <td>t1_gzoh470</td>\n",
       "      <td>t3_nmehx2</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>You got balls buddy! I spend another 1k on amc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>10000</td>\n",
       "      <td>gzoh4a6</td>\n",
       "      <td>2021-05-27 20:30:42</td>\n",
       "      <td>t1_gzoh4a6</td>\n",
       "      <td>t3_nmg57b</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>BEARS BEEN CALLING ME AN IDIOT AND TRYING TO S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10001 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 comment_id              created    fullname   parent_id  \\\n",
       "0               0    gzo4vsv  2021-05-27 19:02:58  t1_gzo4vsv  t1_gzo3wbw   \n",
       "1               1    gzo4vv6  2021-05-27 19:02:58  t1_gzo4vv6  t1_gzo4lej   \n",
       "2               2    gzo4vvp  2021-05-27 19:02:59  t1_gzo4vvp  t1_gzo4r94   \n",
       "3               3    gzo4vwz  2021-05-27 19:02:59  t1_gzo4vwz   t3_nm414o   \n",
       "4               4    gzo4vyx  2021-05-27 19:03:00  t1_gzo4vyx   t3_nm414o   \n",
       "...           ...        ...                  ...         ...         ...   \n",
       "9996         9996    gzoh41e  2021-05-27 20:30:39  t1_gzoh41e   t3_nmg57b   \n",
       "9997         9997    gzoh45k  2021-05-27 20:30:41  t1_gzoh45k  t1_gzogdpm   \n",
       "9998         9998    gzoh461  2021-05-27 20:30:41  t1_gzoh461  t1_gzogoih   \n",
       "9999         9999    gzoh470  2021-05-27 20:30:41  t1_gzoh470   t3_nmehx2   \n",
       "10000       10000    gzoh4a6  2021-05-27 20:30:42  t1_gzoh4a6   t3_nmg57b   \n",
       "\n",
       "            subreddit                                               text  \n",
       "0      wallstreetbets                       Yes. Is it smart? Debatable.  \n",
       "1      wallstreetbets  put a majority of my account into this based o...  \n",
       "2      wallstreetbets                   We are in this togheter brotherü¶ç  \n",
       "3      wallstreetbets                                              CLOV!  \n",
       "4      wallstreetbets               Now do for PLTR what you did for AMC  \n",
       "...               ...                                                ...  \n",
       "9996   wallstreetbets  SPCE baby girls.  I need some hooker money for...  \n",
       "9997   wallstreetbets  I thought about that today too. It has differe...  \n",
       "9998   wallstreetbets          It's been in the $10 range for months tho  \n",
       "9999   wallstreetbets  You got balls buddy! I spend another 1k on amc...  \n",
       "10000  wallstreetbets  BEARS BEEN CALLING ME AN IDIOT AND TRYING TO S...  \n",
       "\n",
       "[10001 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "consecutive-quilt",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4299c43e1411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#insert_submissions(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#COMMENTS subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comments' is not defined"
     ]
    }
   ],
   "source": [
    "## SUBMISSIONS subset\n",
    "#print(len(submissions))\n",
    "#df = submissions[1:500]\n",
    "#insert_submissions(df)\n",
    "#COMMENTS subset\n",
    "#print(len(comments))\n",
    "#df_c = comments[500:5500]\n",
    "#\n",
    "#insert_comments(df_c)\n",
    "\n",
    "#insert_c(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "brutal-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://localhost:5050/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "artistic-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_submissions(dataframe: pd.DataFrame):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        #create submission nodes\n",
    "        url2 = 'submission/'+row['subreddit']+'/'+row['created'].strftime('%m-%d-%Y')+'/'+row['id']\n",
    "        response = requests.post(url+url2)\n",
    "        if not response.status_code == 200:\n",
    "            print ('submission id '+ row['id'] + ' failed insertion')\n",
    "        # create mentions relationships\n",
    "        for org in row['Organizations']:\n",
    "            url3 = 'connection/mentions/submission/'+row['id']+'/'+ org\n",
    "            response = requests.post(url+url3)\n",
    "            if not response.status_code == 200:\n",
    "                print ('mentions between '+ row['id'] + ' and ' + org + ' failed creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amazing-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_comments(dataframe: pd.DataFrame):\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        \n",
    "        #create comment nodes\n",
    "        url2 = 'comment/'+row['subreddit']+'/'+row['created'].strftime('%m-%d-%Y')+'/'+row['comment_id']+'/'+row['parent_id']\n",
    "        response = requests.post(url+url2)\n",
    "        if not response.status_code == 200:\n",
    "            print ('comment id '+ row['comment_id'] + ' failed insertion')\n",
    "            \n",
    "        # create mentions relationships\n",
    "        for org in row['Organizations']:\n",
    "            url3 = 'connection/mentions/comment/'+row['comment_id']+'/'+ org\n",
    "            response = requests.post(url+url3)\n",
    "            if not response.status_code == 200:\n",
    "                print ('mention between '+ row['comment_id'] + ' and ' + org + ' failed creation')\n",
    "    \n",
    "           \n",
    "        # create replies relationships\n",
    "        row['parent_id'] = pd.Series(row['parent_id'], dtype=\"string\") #turning parent_id from type Series to String\n",
    "        for parent_id in row['parent_id']:\n",
    "            p_id = parent_id[3:]\n",
    "            if parent_id[:2] == 't1':\n",
    "                url_reply = 'connection/replies/comment/' + row['comment_id'] + '/' + p_id\n",
    "            elif parent_id[:2] == 't3':\n",
    "                url_reply = 'connection/replies/submission/' + row['comment_id'] + '/' + p_id\n",
    "            response = requests.post(url+url_reply)\n",
    "            if not response.status_code == 200:\n",
    "                print ('replies relationship between '+ row['comment_id'] + ' and ' + p_id + ' failed creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-community",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-silicon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-symphony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-denver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-chicago",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
